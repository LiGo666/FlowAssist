# FlowAssist

A comprehensive AI-powered development environment with Next.js, Model Context Protocol (MCP) Server, analytics capabilities, n8n workflow automation, Ollama LLM server, Qdrant vector database, and PostgreSQL.

## Project Overview

FlowAssist is an integrated development platform that combines modern web technologies with AI capabilities. The platform consists of:

1. **Next.js Frontend**: A modern React-based web application with chat UI, tool toggles, and feedback mechanisms
2. **MCP Server**: Model Context Protocol server for AI orchestration, planning, and tool management
3. **Analytics Service**: Processes telemetry data to generate insights and improve AI performance
4. **n8n Workflow Automation**: With pre-configured AI workflows from the n8n self-hosted AI starter kit
5. **LangChain**: Custom chains and tools for advanced AI processing
6. **Ollama**: Open-source Large Language Model (LLM) server
7. **Qdrant**: Vector database for embeddings and semantic search
8. **PostgreSQL**: Shared database for all services
9. **Shared Memory**: Adapters for Qdrant and Redis for context management

All components are containerized using Docker and managed through Docker Compose, making the setup portable and easy to deploy.

## Project Structure

```
FlowAssist/
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml    # Main Docker Compose configuration
‚îú‚îÄ‚îÄ .env                  # Environment variables (copy from sample.env)
‚îÇ
‚îú‚îÄ‚îÄ nextjs-app/           # Chat UI
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatStream.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ToolTogglePanel.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TraceViewer.tsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ FeedbackPrompt.tsx
‚îÇ   ‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api/ask.ts    # Proxy to MCP
‚îÇ   ‚îî‚îÄ‚îÄ ...               # assets, styles, tests
‚îÇ
‚îú‚îÄ‚îÄ mcp-server/           # Orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ router/           # Dispatch logic
‚îÇ   ‚îú‚îÄ‚îÄ planners/         # (LLM/Rule) task planners
‚îÇ   ‚îú‚îÄ‚îÄ context/          # Profile & memory injectors
‚îÇ   ‚îú‚îÄ‚îÄ tools/            # Registry YAML + adapters
‚îÇ   ‚îú‚îÄ‚îÄ telemetry/        # Trace + metrics exporter
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ analytics-service/    # Kafka-consumer or cron job
‚îÇ   ‚îú‚îÄ‚îÄ pipelines/        # Usage ‚Ü¶ insights ‚Ü¶ model-updates
‚îÇ   ‚îî‚îÄ‚îÄ reports/          # Persisted dashboards
‚îÇ
‚îú‚îÄ‚îÄ n8n/                  # Workflows
‚îÇ
‚îú‚îÄ‚îÄ langchain/            # Custom chains/tools
‚îÇ
‚îú‚îÄ‚îÄ shared-memory/        # Qdrant + Redis adapters
‚îÇ
‚îú‚îÄ‚îÄ qdrant-data/          # Vector database storage
‚îú‚îÄ‚îÄ ollama-data/          # Ollama models and configuration
‚îî‚îÄ‚îÄ pgdata/               # PostgreSQL data directory
```

## Services and Ports

| Service           | Description                                | Port  | URL                    |
|------------------|--------------------------------------------|-------|------------------------|
| Next.js          | Frontend web application                   | 3000  | http://localhost:3000  |
| MCP Server       | Model Context Protocol server              | 3001  | http://localhost:3001  |
| n8n              | Workflow automation with AI capabilities    | 5678  | http://localhost:5678  |
| Ollama           | LLM server running llama3.2                 | 11434 | http://localhost:11434 |
| Qdrant           | Vector database for embeddings             | 6333  | http://localhost:6333  |
| PostgreSQL       | Database server                            | 5432  | localhost:5432         |
| Analytics Service| Telemetry processing and insights          | 8080  | http://localhost:8080  |
| Kafka/Redpanda   | Event streaming for telemetry              | 9092  | localhost:9092         |
| Grafana          | Dashboards for metrics and traces          | 3000  | http://localhost:3000/grafana |

## Functional Principles

### Request Loop (Dynamic Version)

```
User ‚Üî Chat UI
     ‚Üì   ‚Üë (explicit thumbs-up / down)
ask.ts ‚Üí MCP /ask
         ‚îú‚îÄ fetchUserProfile()             (Entra ID / ServiceNow)
         ‚îú‚îÄ loadMemory()                   (Qdrant + Redis)
         ‚îú‚îÄ filterTools(profile,toggles)
         ‚îú‚îÄ plan = {steps[]}               (rule engine ‚à• LLM planner)
         ‚îú‚îÄ execute plan:                 (n8n, LangChain, REST, ‚Ä¶)
         ‚îú‚îÄ trace = {events,latencies}
         ‚îî‚îÄ return {answer, traceId}
UI ‚Üê answer + trace
UI ‚Üí POST /feedback traceId, rating, comment
```

### Feedback-Driven Improvement

- **Immediate**: Rating propagates to analytics topic -> "hot" alert if many failures.
- **Periodic**: Analytics service recomputes tool-success matrix, prompt-quality metrics ‚Üí produces YAML "tuning suggestions" consumed by MCP on redeploy.

### Context Management

- **Static**: Role, department, preferred language (Entra ID).
- **Dynamic**: Active project tickets (ServiceNow), recent docs.
- **Memory**: Last N messages (Redis), long-term embeddings (Qdrant).
- **Injection Rule**: Never exceed context-token-budget; ranking by recency √ó similarity.

### Governance & Security

- Tool registry YAML contains `permissions:` and `rate_limit:`.
- MCP enforces allow/block, logs any violation to telemetry/violations.

### Observability Stack

- Telemetry writes OpenTelemetry spans ‚Üí Jaeger or Tempo.
- LangSmith integrated for LLM calls.
- Grafana dashboards read from PostgreSQL materialized views.

## Getting Started

### Prerequisites

- Docker and Docker Compose installed on your system
- Git for cloning the repository

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/FlowAssist.git
   cd FlowAssist
   ```

2. Copy the sample environment file:
   ```bash
   cp sample.env .env
   ```

3. Start all services:
   ```bash
   docker compose up -d
   ```

4. Access the services:
   - Next.js: http://localhost:3000
   - MCP Server: http://localhost:3001
   - n8n: http://localhost:5678 (default login: admin/password)
   - Ollama: http://localhost:11434
   - Qdrant: http://localhost:6333
   - PostgreSQL: localhost:5432 (username: postgres, password: postgres)

### Initial Setup

- The first time you start the services, Ollama will download the llama3.2 model, which may take some time depending on your internet connection.
- n8n will automatically import the AI workflows from the starter kit.

## Development

### Next.js Frontend

- The Next.js app is mounted as a volume, so changes will be reflected immediately.
- Access the application at http://localhost:3000.
- The app is configured with the standalone output option for optimal Docker deployment.

### MCP Server

- The Model Context Protocol server provides AI model integration capabilities.
- Source files are mounted as volumes for easy development.
- The server is built with TypeScript and runs on Node.js.
- Access the server at http://localhost:3001.

### n8n Workflow Automation

- n8n provides a visual workflow editor for creating automation workflows.
- Pre-configured AI workflows are imported from the n8n self-hosted AI starter kit.
- Access the n8n editor at http://localhost:5678 (login: admin/password).
- Workflows are persisted in the PostgreSQL database and the n8n/.n8n directory.

### Ollama LLM Server

- Ollama provides local LLM capabilities with the llama3.2 model.
- Models are stored in the ollama-data directory.
- Access the Ollama API at http://localhost:11434.
- Example API call: `curl http://localhost:11434/api/generate -d '{"model": "llama3.2", "prompt": "Hello world"}'`

### Qdrant Vector Database

- Qdrant provides vector search capabilities for AI applications.
- Data is stored in the qdrant-data directory.
- Access the Qdrant API at http://localhost:6333.
- Use the Qdrant dashboard at http://localhost:6333/dashboard.

### PostgreSQL Database

- PostgreSQL is used as the shared database for all services.
- Data is persisted in the pgdata directory.
- Connection details:
  - Host: localhost
  - Port: 5432
  - Username: postgres
  - Password: postgres
  - Databases: nextjs_db, n8n

## Environment Variables

Key environment variables (defined in .env):

- `NEXT_PUBLIC_API_URL`: URL for the MCP server (http://localhost:3001)
- `DATABASE_URL`: PostgreSQL connection string
- `N8N_ENCRYPTION_KEY`: Encryption key for n8n
- `N8N_USER_MANAGEMENT_JWT_SECRET`: JWT secret for n8n user management
- `OLLAMA_HOST`: Host for Ollama service (ollama:11434)

## AI Capabilities

### Pre-configured AI Workflows

The n8n instance comes with pre-configured AI workflows from the n8n self-hosted AI starter kit:

1. **Document Q&A**: Ask questions about documents using LLMs
2. **Text Generation**: Generate text using the Ollama LLM
3. **Semantic Search**: Search through documents using vector embeddings

### Integration Points

- Next.js frontend can communicate with the MCP server for AI capabilities
- n8n workflows can use Ollama for LLM processing
- Qdrant provides vector search for semantic queries
- All components can be orchestrated together using n8n workflows

## High-level GUI Mapping

| Screen area | Visible feature | Back-end tie-in |
|-------------|----------------|------------------|
| A. Chat pane | Streaming markdown answers | `/api/ask` SSE |
| B. Tool toggle drawer | Switches per tool, profile selector ("Admin", "Basic") | Stored in `user_tool_prefs` table ‚Üí MCP filter |
| C. Trace timeline | Expandable disclosure list of steps, latencies, tool names | `GET /trace/:id` (MCP telemetry) |
| D. Feedback bar | üëç / üëé + free text ("Hallucination", "Slow", ‚Ä¶) | `POST /feedback` ‚Üí analytics pipeline |
| E. Insights dashboard | (Admin route) aggregated metrics, error clusters | Queries PostgreSQL views / Grafana iframe |

### Wireframe (text)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Chat ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ You ‚ñ∏ "Summarise ticket SN12345"            ‚îÇ
‚îÇ AI  ‚ñ∏ "Here is the summary‚Ä¶"                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
[Trace ‚ñ∏]  [Enable Tool: Calendar ‚úì]  [üëç|üëé]
```

## Extending the Platform

### Adding New Models to Ollama

```bash
# Pull a new model
docker exec -it ollama ollama pull mistral

# List available models
docker exec -it ollama ollama list
```

### Creating Custom n8n Workflows

1. Access the n8n editor at http://localhost:5678
2. Create new workflows using the visual editor
3. Integrate with Ollama and Qdrant for AI capabilities

### Connecting Next.js to AI Services

The Next.js app can connect to:
- MCP server directly via http://mcp:3001
- n8n workflows via http://n8n:5678/webhook/
- Ollama via http://ollama:11434

## Implementation Checklist

The following tasks are required to finalize the framework:

| # | Task | Owner | Status |
|---|------|-------|--------|
| 1 | Extend docker-compose.yml with analytics-service + Kafka | DevOps | ‚òê |
| 2 | Design PostgreSQL schema: traces, tool_usage, feedback | Backend | ‚òê |
| 3 | Implement telemetry/ middleware in MCP (OpenTelemetry SDK) | Backend | ‚òê |
| 4 | Build ToolTogglePanel.tsx + persist prefs via /api/tools | Frontend | ‚òê |
| 5 | Add /feedback endpoint (traceId, rating, comment) | MCP | ‚òê |
| 6 | Create initial Grafana dashboard (latency, fail-rate, NPS) | DevOps | ‚òê |
| 7 | Implement context-ranking function (recency√ósimilarity) | Backend | ‚òê |
| 8 | Integrate LangSmith for all LLM calls | Backend | ‚òê |
| 9 | Write planner stub (rule-based) + optional LangChain agent | AI Eng | ‚òê |
| 10 | Import ServiceNow & Entra ID adapters (profile fetch) | Backend | ‚òê |
| 11 | Security review: role-based tool gating, rate limits | SecOps | ‚òê |
| 12 | End-to-end test script (Cypress) covering toggle ‚Üî trace ‚Üî feedback | QA | ‚òê |
| 13 | Documentation: architecture diagram, Dev guide, Ops playbook | Tech Writer | ‚òê |
| 14 | Pilot rollout: select 10 users, collect feedback | PM | ‚òê |

By completing the checklist you will have:

- Full loop from user request ‚Üí orchestration ‚Üí transparent trace ‚Üí explicit feedback ‚Üí analytics ‚Üí continuous improvement.
- A GUI that surfaces exactly why the assistant acted, and lets users steer capabilities on demand.
- An architecture ready for scaling: add new tools by dropping YAML + adapter, redeploy MCP.

## Troubleshooting

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- [n8n self-hosted AI starter kit](https://github.com/n8n-io/self-hosted-ai-starter-kit)
- [Model Context Protocol (MCP)](https://github.com/microsoft/modelcontextprotocol)
- [Ollama](https://github.com/ollama/ollama)
- [Qdrant](https://github.com/qdrant/qdrant)
- [Next.js](https://nextjs.org/)
- [PostgreSQL](https://www.postgresql.org/)
